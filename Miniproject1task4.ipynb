{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fc4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 21:02:44.472 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/grs9/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-01-22 21:02:44.474 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'st.session_state has no key \"categories\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:398\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(widget_id, key)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:443\u001b[0m, in \u001b[0;36mSessionState._getitem\u001b[0;34m(self, widget_id, user_key)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# We'll never get here\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 355\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# st.selectbox(label=\"Pick the categories you want this search demo to be about...\",\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# options=(\"Flowers Colors Cars Weather Food\", \"Chocolate Milk\", \"Anger Joy Sad Frustration Worry Happiness\", \"Positive Negative\"),\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# key=\"categories\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    352\u001b[0m st\u001b[38;5;241m.\u001b[39mtext_input(\n\u001b[1;32m    353\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategories\u001b[39m\u001b[38;5;124m\"\u001b[39m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlowers Colors Cars Weather Food\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m )\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28mprint\u001b[39m(st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# print(\"Categories = \", categories)\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# st.session_state.categories = categories\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/state/session_state_proxy.py:90\u001b[0m, in \u001b[0;36mSessionStateProxy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     88\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(key)\n\u001b[1;32m     89\u001b[0m require_valid_user_key(key)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_session_state()[key]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/state/safe_session_state.py:91\u001b[0m, in \u001b[0;36mSafeSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_yield_callback()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[key]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/streamlit/runtime/state/session_state.py:400\u001b[0m, in \u001b[0;36mSessionState.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(widget_id, key)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(_missing_key_error_message(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'st.session_state has no key \"categories\". Did you forget to initialize it? More info: https://docs.streamlit.io/library/advanced-features/session-state#initialization'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pickle\n",
    "import os\n",
    "import gdown\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "def cosine_similarity(x, y):\n",
    "    \"\"\"\n",
    "    Exponentiated cosine similarity\n",
    "    1. Compute cosine similarity\n",
    "    2. Exponentiate cosine similarity\n",
    "    3. Return exponentiated cosine similarity\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(x, y)\n",
    "    magnitude_x = np.linalg.norm(x)\n",
    "    magnitude_y = np.linalg.norm(y)\n",
    "    cosine_similarity = dot_product / (magnitude_x * magnitude_y)\n",
    "\n",
    "    # Exponentiate cosine similarity\n",
    "    exp_cosine_similarity = np.exp(cosine_similarity)\n",
    "\n",
    "    # Return exponentiated cosine similarity\n",
    "    return exp_cosine_similarity\n",
    "    \n",
    "\n",
    "# Function to Load Glove Embeddings\n",
    "def load_glove_embeddings(glove_path=\"Data/embeddings.pkl\"):\n",
    "    with open(glove_path, \"rb\") as f:\n",
    "        embeddings_dict = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def get_model_id_gdrive(model_type):\n",
    "    if model_type == \"25d\":\n",
    "        word_index_id = \"13qMXs3-oB9C6kfSRMwbAtzda9xuAUtt8\"\n",
    "        embeddings_id = \"1-RXcfBvWyE-Av3ZHLcyJVsps0RYRRr_2\"\n",
    "    elif model_type == \"50d\":\n",
    "        embeddings_id = \"1DBaVpJsitQ1qxtUvV1Kz7ThDc3az16kZ\"\n",
    "        word_index_id = \"1rB4ksHyHZ9skes-fJHMa2Z8J1Qa7awQ9\"\n",
    "    elif model_type == \"100d\":\n",
    "        word_index_id = \"1-oWV0LqG3fmrozRZ7WB1jzeTJHRUI3mq\"\n",
    "        embeddings_id = \"1SRHfX130_6Znz7zbdfqboKosz-PfNvNp\"\n",
    "        \n",
    "    return word_index_id, embeddings_id\n",
    "\n",
    "\n",
    "def download_glove_embeddings_gdrive(model_type):\n",
    "    # Get glove embeddings from google drive\n",
    "    word_index_id, embeddings_id = get_model_id_gdrive(model_type)\n",
    "\n",
    "    # Use gdown to get files from google drive\n",
    "    embeddings_temp = \"embeddings_\" + str(model_type) + \"_temp.npy\"\n",
    "    word_index_temp = \"word_index_dict_\" + str(model_type) + \"_temp.pkl\"\n",
    "\n",
    "    # Download word_index pickle file\n",
    "    print(\"Downloading word index dictionary....\\n\")\n",
    "    gdown.download(id=word_index_id, output=word_index_temp, quiet=False)\n",
    "\n",
    "    # Download embeddings numpy file\n",
    "    print(\"Donwloading embedings...\\n\\n\")\n",
    "    gdown.download(id=embeddings_id, output=embeddings_temp, quiet=False)\n",
    "\n",
    "\n",
    "# @st.cache_data()\n",
    "def load_glove_embeddings_gdrive(model_type):\n",
    "    word_index_temp = \"word_index_dict_\" + str(model_type) + \"_temp.pkl\"\n",
    "    embeddings_temp = \"embeddings_\" + str(model_type) + \"_temp.npy\"\n",
    "\n",
    "    # Load word index dictionary\n",
    "    word_index_dict = pickle.load(open(word_index_temp, \"rb\"), encoding=\"latin\")\n",
    "\n",
    "    # Load embeddings numpy\n",
    "    embeddings = np.load(embeddings_temp)\n",
    "\n",
    "    return word_index_dict, embeddings\n",
    "\n",
    "\n",
    "@st.cache_resource()\n",
    "def load_sentence_transformer_model(model_name):\n",
    "    sentenceTransformer = SentenceTransformer(model_name)\n",
    "    return sentenceTransformer\n",
    "\n",
    "\n",
    "def get_sentence_transformer_embeddings(sentence, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Get sentence transformer embeddings for a sentence\n",
    "    \"\"\"\n",
    "    # 384 dimensional embedding\n",
    "    # Default model: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  \n",
    "\n",
    "    sentenceTransformer = load_sentence_transformer_model(model_name)\n",
    "\n",
    "    try:\n",
    "        return sentenceTransformer.encode(sentence)\n",
    "    except:\n",
    "        if model_name == \"all-MiniLM-L6-v2\":\n",
    "            return np.zeros(384)\n",
    "        else:\n",
    "            return np.zeros(512)\n",
    "\n",
    "\n",
    "def get_glove_embeddings(word, word_index_dict, embeddings, model_type):\n",
    "    \"\"\"\n",
    "    Get glove embedding for a single word\n",
    "    \"\"\"\n",
    "    if word.lower() in word_index_dict:\n",
    "        return embeddings[word_index_dict[word.lower()]]\n",
    "    else:\n",
    "        return np.zeros(int(model_type.split(\"d\")[0]))\n",
    "\n",
    "\n",
    "def averaged_glove_embeddings_gdrive(sentence, word_index_dict, embeddings, model_type=50):\n",
    "    \"\"\"\n",
    "    Get averaged glove embeddings for a sentence\n",
    "    1. Split sentence into words\n",
    "    2. Get embeddings for each word\n",
    "    3. Add embeddings for each word\n",
    "    4. Divide by number of words\n",
    "    5. Return averaged embeddings\n",
    "    \"\"\"\n",
    "    # Initialize an embedding vector with zeros\n",
    "    embedding_dim = int(model_type.split(\"d\")[0])\n",
    "    embedding = np.zeros(embedding_dim)\n",
    "\n",
    "    # Split sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Keep track of the number of words found in the embedding dictionary\n",
    "    valid_words_count = 0\n",
    "\n",
    "    # Iterate over each word in the sentence\n",
    "    for word in words:\n",
    "        # Check if the word is in the word index dictionary\n",
    "        if word in word_index_dict:\n",
    "            # Add the embedding for this word\n",
    "            embedding += embeddings[word_index_dict[word]]\n",
    "            valid_words_count += 1\n",
    "\n",
    "    # Check if there were valid words to avoid division by zero\n",
    "    if valid_words_count > 0:\n",
    "        # Divide by the number of valid words to get the average\n",
    "        embedding /= valid_words_count\n",
    "\n",
    "    # Return the averaged embeddings\n",
    "    return embedding\n",
    "\n",
    "def get_category_embeddings(embeddings_metadata):\n",
    "    \"\"\"\n",
    "    Get embeddings for each category\n",
    "    1. Split categories into words\n",
    "    2. Get embeddings for each word\n",
    "    \"\"\"\n",
    "    model_name = embeddings_metadata[\"model_name\"]\n",
    "    st.session_state[\"cat_embed_\" + model_name] = {}\n",
    "    for category in st.session_state.categories.split(\" \"):\n",
    "        if model_name:\n",
    "            if not category in st.session_state[\"cat_embed_\" + model_name]:\n",
    "                st.session_state[\"cat_embed_\" + model_name][category] = get_sentence_transformer_embeddings(category, model_name=model_name)\n",
    "        else:\n",
    "            if not category in st.session_state[\"cat_embed_\" + model_name]:\n",
    "                st.session_state[\"cat_embed_\" + model_name][category] = get_sentence_transformer_embeddings(category)\n",
    "\n",
    "\n",
    "def update_category_embeddings(embedings_metadata):\n",
    "    \"\"\"\n",
    "    Update embeddings for each category\n",
    "    \"\"\"\n",
    "    get_category_embeddings(embeddings_metadata)\n",
    "\n",
    "\n",
    "def get_sorted_cosine_similarity(embeddings_metadata):\n",
    "    \"\"\"\n",
    "    Get sorted cosine similarity between input sentence and categories\n",
    "    Steps:\n",
    "    1. Get embeddings for input sentence\n",
    "    2. Get embeddings for categories (if not found, update category embeddings)\n",
    "    3. Compute cosine similarity between input sentence and categories\n",
    "    4. Sort cosine similarity\n",
    "    5. Return sorted cosine similarity\n",
    "    (50 pts)\n",
    "    \"\"\"\n",
    "    # Ask TA\n",
    "    categories = st.session_state.categories.split(\" \")\n",
    "    cosine_sim = {}\n",
    "    category_embeddings\n",
    "    input_embedding\n",
    "    if embeddings_metadata[\"embedding_model\"] == \"glove\":\n",
    "        word_index_dict = embeddings_metadata[\"word_index_dict\"]\n",
    "        embeddings = embeddings_metadata[\"embeddings\"]\n",
    "        model_type = embeddings_metadata[\"model_type\"]\n",
    "\n",
    "        input_embedding = averaged_glove_embeddings_gdrive(st.session_state.text_search,\n",
    "                                                            word_index_dict,\n",
    "                                                            embeddings, model_type)\n",
    "        \n",
    "        ##########################################\n",
    "        ## TODO: Get embeddings for categories ###\n",
    "        ##########################################\n",
    "        st.session_state[\"cat_embed_\" + model_name] = {}\n",
    "        if categories != None:\n",
    "            # Get and compute embeddings for each category\n",
    "            for category in categories:\n",
    "                category_embeddings.append(averaged_glove_embeddings_gdrive(category,word_index_dict,embeddings, model_type))\n",
    "\n",
    "\n",
    "    else:\n",
    "        model_name = embeddings_metadata[\"model_name\"]\n",
    "        if not \"cat_embed_\" + model_name in st.session_state:\n",
    "            get_category_embeddings(embeddings_metadata)\n",
    "\n",
    "        category_embeddings = st.session_state[\"cat_embed_\" + model_name]\n",
    "\n",
    "        print(\"text_search = \", st.session_state.text_search)\n",
    "        if model_name:\n",
    "            input_embedding = get_sentence_transformer_embeddings(st.session_state.text_search, model_name=model_name)\n",
    "        else:\n",
    "            input_embedding = get_sentence_transformer_embeddings(st.session_state.text_search)\n",
    "    \n",
    "    for index in range(len(categories)):\n",
    "        ##########################################\n",
    "        # TODO: Compute cosine similarity between input sentence and categories\n",
    "        # TODO: Update category embeddings if category not found\n",
    "        ##########################################\n",
    "        category_embedding = category_embeddings[index]\n",
    "        cosine_sim[categories[index]] = cosine_similarity(input_embedding, category_embedding)\n",
    "    \n",
    "    # Sort cosine similarities in descending order\n",
    "    sorted_cosine_sim = sorted(cosine_sim.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    return sorted_cosine_sim\n",
    "\n",
    "\n",
    "def plot_piechart(sorted_cosine_scores_items):\n",
    "    sorted_cosine_scores = np.array([\n",
    "            sorted_cosine_scores_items[index][1]\n",
    "            for index in range(len(sorted_cosine_scores_items))\n",
    "        ]\n",
    "    )\n",
    "    categories = st.session_state.categories.split(\" \")\n",
    "    categories_sorted = [\n",
    "        categories[sorted_cosine_scores_items[index][0]]\n",
    "        for index in range(len(sorted_cosine_scores_items))\n",
    "    ]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(sorted_cosine_scores, labels=categories_sorted, autopct=\"%1.1f%%\")\n",
    "    st.pyplot(fig)  # Figure\n",
    "\n",
    "\n",
    "def plot_piechart_helper(sorted_cosine_scores_items):\n",
    "    sorted_cosine_scores = np.array(\n",
    "        [\n",
    "            sorted_cosine_scores_items[index][1]\n",
    "            for index in range(len(sorted_cosine_scores_items))\n",
    "        ]\n",
    "    )\n",
    "    categories = st.session_state.categories.split(\" \")\n",
    "    categories_sorted = [\n",
    "        categories[sorted_cosine_scores_items[index][0]]\n",
    "        for index in range(len(sorted_cosine_scores_items))\n",
    "    ]\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    my_explode = np.zeros(len(categories_sorted))\n",
    "    my_explode[0] = 0.2\n",
    "    if len(categories_sorted) == 3:\n",
    "        my_explode[1] = 0.1  # explode this by 0.2\n",
    "    elif len(categories_sorted) > 3:\n",
    "        my_explode[2] = 0.05\n",
    "    ax.pie(\n",
    "        sorted_cosine_scores,\n",
    "        labels=categories_sorted,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        explode=my_explode,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_piecharts(sorted_cosine_scores_models):\n",
    "    scores_list = []\n",
    "    categories = st.session_state.categories.split(\" \")\n",
    "    index = 0\n",
    "    for model in sorted_cosine_scores_models:\n",
    "        scores_list.append(sorted_cosine_scores_models[model])\n",
    "        # scores_list[index] = np.array([scores_list[index][ind2][1] for ind2 in range(len(scores_list[index]))])\n",
    "        index += 1\n",
    "\n",
    "    if len(sorted_cosine_scores_models) == 2:\n",
    "        fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "        categories_sorted = [\n",
    "            categories[scores_list[0][index][0]] for index in range(len(scores_list[0]))\n",
    "        ]\n",
    "        sorted_scores = np.array(\n",
    "            [scores_list[0][index][1] for index in range(len(scores_list[0]))]\n",
    "        )\n",
    "        ax1.pie(sorted_scores, labels=categories_sorted, autopct=\"%1.1f%%\")\n",
    "\n",
    "        categories_sorted = [\n",
    "            categories[scores_list[1][index][0]] for index in range(len(scores_list[1]))\n",
    "        ]\n",
    "        sorted_scores = np.array(\n",
    "            [scores_list[1][index][1] for index in range(len(scores_list[1]))]\n",
    "        )\n",
    "        ax2.pie(sorted_scores, labels=categories_sorted, autopct=\"%1.1f%%\")\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "\n",
    "def plot_alatirchart(sorted_cosine_scores_models):\n",
    "    models = list(sorted_cosine_scores_models.keys())\n",
    "    tabs = st.tabs(models)\n",
    "    figs = {}\n",
    "    for model in models:\n",
    "        figs[model] = plot_piechart_helper(sorted_cosine_scores_models[model])\n",
    "\n",
    "    for index in range(len(tabs)):\n",
    "        with tabs[index]:\n",
    "            st.pyplot(figs[models[index]])\n",
    "\n",
    "\n",
    "### Text Search ###\n",
    "st.sidebar.title(\"GloVe Twitter\")\n",
    "st.sidebar.markdown(\n",
    "    \"\"\"\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Pretrained on \n",
    "2 billion tweets with vocabulary size of 1.2 million. Download from [Stanford NLP](http://nlp.stanford.edu/data/glove.twitter.27B.zip). \n",
    "\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. *GloVe: Global Vectors for Word Representation*.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model_type = st.sidebar.selectbox(\"Choose the model\", (\"25d\", \"50d\"), index=1)\n",
    "\n",
    "\n",
    "st.title(\"Search Based Retrieval Demo\")\n",
    "st.subheader(\n",
    "    \"Pass in space separated categories you want this search demo to be about.\"\n",
    ")\n",
    "# st.selectbox(label=\"Pick the categories you want this search demo to be about...\",\n",
    "# options=(\"Flowers Colors Cars Weather Food\", \"Chocolate Milk\", \"Anger Joy Sad Frustration Worry Happiness\", \"Positive Negative\"),\n",
    "# key=\"categories\"\n",
    "# )\n",
    "st.text_input(\n",
    "    label=\"Categories\", key=\"categories\", value=\"Flowers Colors Cars Weather Food\"\n",
    ")\n",
    "print(st.session_state[\"categories\"])\n",
    "print(type(st.session_state[\"categories\"]))\n",
    "# print(\"Categories = \", categories)\n",
    "# st.session_state.categories = categories\n",
    "\n",
    "st.subheader(\"Pass in an input word or even a sentence\")\n",
    "text_search = st.text_input(\n",
    "    label=\"Input your sentence\",\n",
    "    key=\"text_search\",\n",
    "    value=\"Roses are red, trucks are blue, and Seattle is grey right now\",\n",
    ")\n",
    "# st.session_state.text_search = text_search\n",
    "\n",
    "# Download glove embeddings if it doesn't exist\n",
    "embeddings_path = \"embeddings_\" + str(model_type) + \"_temp.npy\"\n",
    "word_index_dict_path = \"word_index_dict_\" + str(model_type) + \"_temp.pkl\"\n",
    "if not os.path.isfile(embeddings_path) or not os.path.isfile(word_index_dict_path):\n",
    "    print(\"Model type = \", model_type)\n",
    "    glove_path = \"Data/glove_\" + str(model_type) + \".pkl\"\n",
    "    print(\"glove_path = \", glove_path)\n",
    "\n",
    "    # Download embeddings from google drive\n",
    "    with st.spinner(\"Downloading glove embeddings...\"):\n",
    "        download_glove_embeddings_gdrive(model_type)\n",
    "\n",
    "\n",
    "# Load glove embeddings\n",
    "word_index_dict, embeddings = load_glove_embeddings_gdrive(model_type)\n",
    "\n",
    "\n",
    "# Find closest word to an input word\n",
    "if st.session_state.text_search:\n",
    "    # Glove embeddings\n",
    "    print(\"Glove Embedding\")\n",
    "    embeddings_metadata = {\n",
    "        \"embedding_model\": \"glove\",\n",
    "        \"word_index_dict\": word_index_dict,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"model_type\": model_type,\n",
    "    }\n",
    "    with st.spinner(\"Obtaining Cosine similarity for Glove...\"):\n",
    "        sorted_cosine_sim_glove = get_sorted_cosine_similarity(\n",
    "            st.session_state.text_search, embeddings_metadata\n",
    "        )\n",
    "\n",
    "    # Sentence transformer embeddings\n",
    "    print(\"Sentence Transformer Embedding\")\n",
    "    embeddings_metadata = {\"embedding_model\": \"transformers\", \"model_name\": \"\"}\n",
    "    with st.spinner(\"Obtaining Cosine similarity for 384d sentence transformer...\"):\n",
    "        sorted_cosine_sim_transformer = get_sorted_cosine_similarity(\n",
    "            st.session_state.text_search, embeddings_metadata\n",
    "        )\n",
    "\n",
    "    # Results and Plot Pie Chart for Glove\n",
    "    print(\"Categories are: \", st.session_state.categories)\n",
    "    st.subheader(\n",
    "        \"Closest word I have between: \"\n",
    "        + st.session_state.categories\n",
    "        + \" as per different Embeddings\"\n",
    "    )\n",
    "\n",
    "    print(sorted_cosine_sim_glove)\n",
    "    print(sorted_cosine_sim_transformer)\n",
    "    # print(sorted_distilbert)\n",
    "    # Altair Chart for all models\n",
    "    plot_alatirchart(\n",
    "        {\n",
    "            \"glove_\" + str(model_type): sorted_cosine_sim_glove,\n",
    "            \"sentence_transformer_384\": sorted_cosine_sim_transformer,\n",
    "        }\n",
    "    )\n",
    "    # \"distilbert_512\": sorted_distilbert})\n",
    "\n",
    "    st.write(\"\")\n",
    "    st.write(\n",
    "        \"Demo developed by [Dr. Karthik Mohan Mohan](https://www.linkedin.com/in/karthik-mohan-72a4b323/)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb27eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/e9/07/63a6e890c9b998a6318b46c2a34377fd1a3e01a94c427d82bfb2472b7c16/streamlit-1.30.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading streamlit-1.30.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Obtaining dependency information for altair<6,>=4.0 from https://files.pythonhosted.org/packages/c5/e4/7fcceef127badbb0d644d730d992410e4f3799b295c9964a172f92a469c7/altair-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Obtaining dependency information for blinker<2,>=1.0.0 from https://files.pythonhosted.org/packages/fa/2a/7f3714cbc6356a0efec525ce7a0613d581072ed6eb53eb7b9754f33db807/blinker-1.7.0-py3-none-any.whl.metadata\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Obtaining dependency information for cachetools<6,>=4.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (1.24.3)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (9.4.0)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20 from https://files.pythonhosted.org/packages/b3/81/0017aefacf23273d4efd1154ef958a27eed9c177c4cc09d2d4ba398fb47f/protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pyarrow>=6.0 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Obtaining dependency information for rich<14,>=10.14.0 from https://files.pythonhosted.org/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (4.7.1)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Obtaining dependency information for tzlocal<6,>=1.1 from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Obtaining dependency information for validators<1,>=0.2 from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Obtaining dependency information for gitpython!=3.1.19,<4,>=3.0.7 from https://files.pythonhosted.org/packages/45/c6/a637a7a11d4619957cb95ca195168759a4502991b1b91c13d3203ffc3748/GitPython-3.1.41-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in ./anaconda3/lib/python3.11/site-packages (from streamlit) (6.3.2)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in ./anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in ./anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/lib/python3.11/site-packages (from importlib-metadata<8,>=1.4->streamlit) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in ./anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Downloading streamlit-1.30.0-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: validators, tzlocal, smmap, protobuf, cachetools, blinker, rich, pydeck, gitdb, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.2.0 blinker-1.7.0 cachetools-5.3.2 gitdb-4.0.11 gitpython-3.1.41 protobuf-4.25.2 pydeck-0.8.1b0 rich-13.7.0 smmap-5.0.1 streamlit-1.30.0 tzlocal-5.2 validators-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08992f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.11/site-packages (1.24.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecd66e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for torch>=1.6.0 from https://files.pythonhosted.org/packages/dd/a0/6dd8662895ddaffb0ac689037451f93a73e9030d07b1b5965e3e3071f873/torch-2.1.2-cp311-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading torch-2.1.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision (from sentence-transformers)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/3b/0a/59f82c490e82c3f3671deb374b1d53e3285ff91b76cae46f99b5b3c3e2d7/torchvision-0.16.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata\n",
      "  Downloading torchvision-0.16.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in ./anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./anaconda3/lib/python3.11/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Downloading torch-2.1.2-cp311-none-macosx_10_9_x86_64.whl (146.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp311-cp311-macosx_10_13_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=4c213cf8a942308695387b2f6ffd2718badfcfeb66b88224487c504cb5b01a16\n",
      "  Stored in directory: /Users/grs9/Library/Caches/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, torch, torchvision, sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99 torch-2.1.2 torchvision-0.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa60bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/68/fb/c1bb2cfbf1ad068129e3d67f3420649d38183cca7118f4fa46cfe3c3adab/gdown-5.0.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.11/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in ./anaconda3/lib/python3.11/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.11/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.0.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ccc2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./anaconda3/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60151e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
